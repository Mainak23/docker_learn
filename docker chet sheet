FROM rockylinux:9

RUN dnf install -y python3 && dnf clean all

WORKDIR /app
COPY index.html .

EXPOSE 8000
CMD ["python3", "-m", "http.server", "8000","--bind", "0.0.0.0"]

case 2
FROM rockylinux:9

RUN dnf install -y python3 && dnf clean all

WORKDIR /app
COPY test_folder/font_end .-------->inside docker copy everything inside font_end not font_end folder

EXPOSE 8000
CMD ["bash", "-c", "cd html_tag && python3 -m http.server 8000 --bind 0.0.0.0"]















1. Container lifecycle
Image → Created → Running → Paused → Exited → Removed
Image = immutable blueprint
Container = running process + writable layer
PID 1 controls container life
If PID 1 exits → container stops
run = create + start
exec = enter existing container


Containers need:
Read-only image layers
One writable layer
Zero copying
Fast startup
OverlayFS gives a union filesystem.



image comands
podman history my1stimage
podman inspect my1stimage


cointainer commandsd
podman ps
podman ps -a
podman run -it  e14d8cf0c63b (interactive)

volume commands


Build comand
podman build . -t myfirstimage


run comand
podman run -it   -p 8080:8000   b351a8ce3ab6 --------------> 8080 is host post where podman transfer it trafics from 8000 (inside podman port)
podman run -it e14d8cf0c63b bash -c "cd font_end && exec bash"

"""
Why exec bash matters
Replaces subshell
Keeps session alive
Correct PID 1 behavior
"""




path 

FROM rockylinux:9

#RUN dnf install -y python3 && dnf clean all

#WORKDIR /app --->not createing 
COPY test_folder .

EXPOSE 8000

podman run -it 513bfbbac171
[root@54e773483ab7 /]# ls
afs  bin  dev  etc  font_end  home  lib  lib64	lost+found  main.py  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var ----> find out file inside test folder not test folder

case 2
FROM rockylinux:9

#RUN dnf install -y python3 && dnf clean all

WORKDIR /app
COPY test_folder .

EXPOSE 8000
CMD ["bash", "-c", "cd font_end && python3 -m http.server 8000 --bind 0.0.0.0"]


Key rule:
Each comma-separated item = one argv element
No shell parsing happens unless you explicitly start a shell

Volume in podman 

Why volumes exist (first principles)
A container is ephemeral.
Container writable layer = temporary
podman rm → all container data is gone
Image layers = read-only
So the problem is:
Where do we keep data that must survive container restarts, upgrades, crashes?
Answer: Volumes (or bind mounts).

Without volume (default behavior)

Image (read-only layers)
↓
Container writable layer (OverlayFS upperdir)

If your app writes to /data:
Data lives in container’s writable layer
Stored in OverlayFS upperdir
When container is removed → data is deleted

OverlayFS
OverlayFS is a Linux kernel filesystem, not a container feature.
Its job:
Merge multiple directories into one virtual filesystem

| Term     | Meaning                            |
| -------- | ---------------------------------- |
| lowerdir | One or more read-only directories  |
| upperdir | Writable directory                 |
| workdir  | Scratch space (required by kernel) |
| merged   | What processes see                 |


ML workloads do:
Large sequential reads (datasets, checkpoints)
Massive small writes (logs, temp files, tensor caches)
Frequent metadata ops (stat, open, close)


How Podman uses OverlayFS
Podman itself does not implement OverlayFS.
Podman only:
Prepares directories
Asks the kernel to mount OverlayFS
Manages lifecycle

OverlayFS is optimized for:
Read-heavy, write-light
App binaries, not data pipelines

OverlayFS is fine for:
Python code
Binaries
Small config files

OverlayFS is bad for:
ML data
Databases
Logs
Anything stateful


With volume (what actually happens)
Example:
podman run -v myvol:/app/data myimage

This means:

Host volume (real filesystem)
        ↑
      mounted
        ↑
Container path /app/data

Important rule (this answers your question directly):
Data is NOT copied into the container.
The container sees the host data directly.

Can you open these files directly?
❌ No
They are:
Binary
Page-based
Engine-version specific
Checksummed and encrypted internally

Where you SHOULD mount container storage (best practice)
❌ Never use
/
/boot
tmpfs

✅ Best locations
Location	When to use
/mnt/docker-data	External disk
/data/containers	Separate internal disk
/srv/containers	Server-standard layout

When you should NOT use containers for DBs
If:
Single server
One SQL Server instance
Stable schema
No frequent redeployments
No CI/CD
Bare-metal performance matters

When containerized DB makes sense for you
Use Podman/Docker if:
You want clean rollback
You want staging/prod parity
You deploy frequently
You plan Kubernetes later
You want infra discipline
Don’t use it if:
One DB
Rare changes
Performance is king
No automation

########
network
#########
network command manages networks for Podman

Podman has two network backends:
Netavark (new) and CNI (old).
Netavark is the default from Podman 4.0.
CNI is deprecated and will be removed in Podman 5.0.
Network CLI commands stay the same, but
Network config files are different, so networks must be recreated if you switch backends.
Backend is set in containers.conf.

Podman ≤3.x → CNI
Podman 4.x+ → Netavark
Podman 5.0 → CNI gone

Production rule (write this down)
Never switch network backends on a running system without recreating networks and containers.

Ignoring this causes:
Broken DNS
Containers not talking
Silent failures


Check current backend with:
podman --version
podman info --format '{{.Host.NetworkBackend}}'
If you see error
3.x → this error is expected
4.x+ → system packages may be mismatched

Podman 3.x (CNI only)
There is no Netavark in Podman 3.x.
So your backend is CNI by definition.
Confirm by checking CNI config:
ls ~/.config/cni/net.d/
----> 87-podman.conflist  cni.lock

What is 87-podman.conflist (important)
This file defines:

Bridge network
Subnet (e.g. 10.88.0.0/16)
Gateway
IPAM rules
Firewall + port mapping plugins
It is the default Podman network.

| Situation                      | Result              |
| ------------------------------ | ------------------- |
| You upgrade to Podman 4/5      | CNI ignored         |
| Containers rely on old network | They fail           |
| DNS resolution breaks          | Silent errors       |
| You don’t recreate networks    | Debugging nightmare |

CNI configs ≠ Netavark configs. Same CLI, different engine.

If you upgrade from Podman 3 → 4:
Existing CNI networks WILL NOT work
You must recreate networks
Containers depending on old networks will fail

CNI and Netavark solve similar-looking problems but are built for different worlds.

CNI = cluster networking (Kubernetes)
Netavark = local container networking (Podman)

Conceptual difference (WHY they exist)
CNI (Container Network Interface)

Goal:
“When a container (pod) starts, tell the system how to connect it to the cluster network.”

Key idea:
CNI is a specification, not a tool
Kubernetes calls CNI plugins

Plugins decide:
IP assignment
Routing
Firewall rules

CNI assumes:
Many nodes
Flat network
No NAT between pods
Scale first, simplicity second

Netavark
Goal:
“Give local containers working networking with minimal overhead.”

Key idea:
Netavark is a single-purpose engine
Written specifically for Podman
Opinionated and simple

Netavark assumes:
Single machine
Containers come and go fast
NAT is acceptable
Developer/ops convenience matters

| Aspect            | CNI (K8s)           | Netavark     |
| ----------------- | ------------------- | ------------ |
| Pod-to-pod        | Direct IP           | Often NAT    |
| Cross-node        | Required            | Not needed   |
| Load balancing    | Service abstraction | Port mapping |
| Performance focus | Scale               | Simplicity   |

Failure model (production insight)
CNI failures:
Pod stuck in ContainerCreating
Node-specific
Hard to debug
Often invisible at first

Netavark failures:
Container won’t start
Local and obvious
Easy to reproduce

NAT 
NAT (Network Address Translation) is:
A technique where a router rewrites IP addresses (and ports) so many private machines can share one public IP.

Problem:
IPv4 has limited addresses
Private networks use non-routable IPs:
10.x.x.x
172.16–31.x.x
192.168.x.x

These cannot travel on the internet.

Solution:
➡️ NAT sits at the boundary and translates them.

Mental model (lock this in)
NAT differentiates machines using PORTS.
Kubernetes differentiates pods using IPs.



They are differentiated by PORT NUMBERS, not just IP addresses.
IP + port together uniquely identify a connection.

Step 1️⃣ What actually identifies a connection
A network connection is identified by this 5-tuple:
(Source IP, Source Port, Destination IP, Destination Port, Protocol)

Example:
(192.168.1.10, 43210, 8.8.8.8, 53, UDP)

Step 2️⃣ What NAT really does (important correction)
NAT does NOT only change the IP.
It usually does PAT (Port Address Translation).

So it changes:
Source IP
Source Port

This is why thousands of machines can share one IP.

Step 3️⃣ Concrete example (two pods, same public IP)

Assume two pods:

Pod	Private IP	Source Port
Pod A	10.88.0.2	40001
Pod B	10.88.0.3	40002

Both access:

8.8.8.8:53

Before NAT
10.88.0.2:40001 → 8.8.8.8:53
10.88.0.3:40002 → 8.8.8.8:53

After NAT (same public IP)
203.0.113.5:55001 → 8.8.8.8:53
203.0.113.5:55002 → 8.8.8.8:53


NAT table:
203.0.113.5:55001 ↔ 10.88.0.2:40001
203.0.113.5:55002 ↔ 10.88.0.3:40002


When replies come back:
Port 55001 → Pod A
Port 55002 → Pod B

Step 4️⃣ Why ports are enough

TCP has 65,535 ports
UDP has 65,535 ports

NAT can reuse ports across different destinations
So a single IP can support millions of concurrent connections.

Step 5️⃣ Incoming traffic (important limitation)

Here’s the catch:
❌ Incoming traffic cannot magically know which pod to reach.

That’s why we need:
DNAT / Port Forwarding
PublicIP:80 → PodA:8080


Choose the most isolated mode that still works for your app.

| Mode         | Isolation | NAT | Performance | Use case        |
| ------------ | --------- | --- | ----------- | --------------- |
| bridge       | High      | Yes | Medium      | Default         |
| host         | None      | No  | Very high   | Monitoring      |
| none         | Total     | No  | N/A         | Secure jobs     |
| user network | High      | Yes | Medium      | Microservices   |
| slirp4netns  | Medium    | Yes | Low         | Rootless        |
| pasta        | Medium    | Yes | High        | Rootless (best) |


Compose

| Concept   | What it does                                           | When you use it                                          |
| --------- | ------------------------------------------------------ | -------------------------------------------------------- |
| `build`   | Creates an **image** of your app (e.g., Flask API)     | When you have custom code                                |
| `compose` | Launches **containers**, networks them, sets env/ports | When you want multi-container setup or service discovery |

flask-api/
  Dockerfile
  requirements.txt 
  app.py
compose.yml

proces
podman build -t myflaskapi .
podman-compose up -d

run
podman exec -it client sh
curl http://api:5000


Focus on:
How pod IPs are assigned
How pods talk across nodes
How services route traffic
How DNS works (CoreDNS)














#####################3
podman build --help

http://localhost:8080/

Full Podman reset (containers + images + networks + cache)
podman system reset -f
####################

Multi-stage 
Multi-stage builds are mandatory in prod
Single-stage images = amateur
Smaller images = faster deploy + safer
If you need bash in runtime, rethink design














